\chapter{Theoretical background}
\label{chap:background}

Aici un rezumat al capitolului.
Este descrisa fiecare sectiune.
In Section \ref{sec:section1} blabla.

\section{Section 1}
\label{sec:section1}


A neural network classifier may be considered as a mapping
\begin{align}
F: R^{d} \rightarrow R^{M},
\end{align}
where $x$ represents the input of the network and the output $y$ is the classification result.

Pentru a folosi modul matematic - cu beginalign sau intre dous semne dolar.

\begin{figure}[h!]
\label{fig:nn}
\centerline{\psfig{figure=nn.eps,width=0.5\textwidth}}
      \caption[Neural network]{A two-layer feed-forward neural network. Image source: \cite{Han06DataMining}.}
\label{fig:decisiontree}
\end{figure}

A multilayer feed-forward neural network is shown in Figure \ref{fig:nn}.

In caption-ul figurii la image source trebuie citata cartea \cite{Han06DataMining} sau articolul de unde este luata figura sau se scrie link-ul catre figura.


A good clustering algorithm should satisfy several requirements as follows \cite{website:clustering}:

\begin{itemize}
\item scalability, i.e., it should still work properly when applied to large datasets
\item ability to handle different types of features, namely, continuous, binary, categorical, ordinal and ratio-scaled
\item tolerance to high dimensional data
\item independence with respect to the cluster shape
\item minimal domain knowledge requirements, i.e., not too many parameters that need to be initialized
\item tolerance to noise
\item tolerance to outliers
\item insensitivity to the order in which data items are read
\item interoperability
\item usability.
\end{itemize}

\begin{definition}
\label{def:entropy}
The expected information needed for classifying a data item in $D$ (the entropy of $D$) is given by
\begin{align}
Info(D) = - \sum_{i=1}^{m}p_{i}\log_{2}(p_{i}),
\end{align}
where:
\begin{itemize}
\item $p_{i}$ denotes the probability that a data item from the dataset belongs to the class $C_{i}$
\item $p_{i} \approx \frac{\mid C_{i} \mid }{\mid D \mid} $.
\end{itemize}
\end{definition}

According to Definition \ref{def:entropy} blabla.

\begin{remark}
\label{rem:purity}
The goal is to find the attribute $A$ from which $Info_{A}(D)$ is minimal and hence the purity of the obtained partitions is maximal.
\end{remark}

According to Remark \ref{rem:purity} bla.

Bayesian classification is a statistical classification method which computes the probability that a given data item belongs to a particular class thus predicting class memberships \cite{Ben08Encyclopedia}.

\begin{theorem}[Bayes' theorem]. Let us denote by $C_j , j=\overline{1,J}$ the possible classes and by $P(C_{j} \mid X_{1},X_{2}, \dots , X_{p})$
the posterior probability of belonging to the  class $C_j$ given the features $X_{1} , X_{2} , \dots , X_{p}$ then
\begin{align*}
P(C_{j} \mid X_{1},X_{2}, \dots , X_{p}) = \frac{P( X_{1} , X_{2} , \dots, X_{p} \mid C_{j} ) \cdot P(C_{j})
}{\sum_{j}P( X_{1} , X_{2} , \dots, X_{p} \mid C_{j})},
\end{align*}
where:
\begin{itemize}
\item $P( X_{1} , X_{2} , \dots, X_{p} \mid C_{j} )$ denotes the probability of an item with individual characteristics (features) $X_{1} , X_{2} , \dots , X_{p}$ belonging to the class $C_j$
\item  $P(C_{j} )$ denotes the unconditional prior probability of belonging to the class $C_j$.
\end{itemize}
\end{theorem}

The goal of the rule based classifier is to construct the smallest set of rules such that consistency with respect to training data is preserved. A large number of rules is an indication of attempting to remember the training set, as opposed to discovering the assumptions that govern it \cite{Kotsiantis07Supervised}. A general pseudo-code for rule-based classifiers is presented in Algorithm ~\ref{alg:rulebased}.

\begin{algorithm}
\caption{Learn rules}
\label{alg:rulebased}
\begin{algorithmic}[1]
\STATE $RuleSet \leftarrow \emptyset$
\FORALL {classes c}
	\REPEAT
		\item $Rule \leftarrow Find\_Best\_Rule(c)$
		\item Remove items covered by $Rule$
	\UNTIL {$termination\_condition$}
	\STATE $RuleSet \leftarrow RuleSet \cup \{Rule\} $
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{table}[h!]
\begin{tabular}{lllll}
\cline{1-5}
MisclassificationId & Similarity  & C1 & C2 & C3\\
\hline
106 & 0.25 & 0 & 0.95 & 0.84\\
119 & 0.24 & 0 & 0.96 & 0.83\\
133 & 0.19 & 0 & 1    & 0.89\\
134 & 0.24 & 0 & 0.96 & 0.82\\
\hline \\
\end{tabular}
\caption{Cluster2 --- RepresentativeId (95)}
\label{tab:cluster2id95}
\end{table}

From the table~\ref{tab:cluster2id95} blabla.


